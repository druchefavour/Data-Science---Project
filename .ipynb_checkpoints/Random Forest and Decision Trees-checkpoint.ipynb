{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A random forest classifier.\n",
    "In this section, a random forest will be used to fit a subsample of the natural gas dataset. A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is always the same as the original input sample size but the samples are drawn with replacement if bootstrap=True (default). The Random Forest algorithm is an averaging algorithm based on randomized decision trees.\n",
    "\n",
    "Using the natural gas data set, we will fit forest classifiers with two arrays: an array X of gas price predictors holding the training samples, and an array Y of gas prices holding the target vawelues (class labels) for the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Days</th>\n",
       "      <th>Date</th>\n",
       "      <th>AveCoalPrice</th>\n",
       "      <th>OilPrice</th>\n",
       "      <th>GrossGasProd</th>\n",
       "      <th>TotGasCons</th>\n",
       "      <th>GasPrice</th>\n",
       "      <th>Weather</th>\n",
       "      <th>WSTAT</th>\n",
       "      <th>GasPriceStatus</th>\n",
       "      <th>GPSAT</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>57.22</td>\n",
       "      <td>41.12</td>\n",
       "      <td>2227.028</td>\n",
       "      <td>2399.702</td>\n",
       "      <td>5.82</td>\n",
       "      <td>WINTER</td>\n",
       "      <td>1</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276</td>\n",
       "      <td>2009-01-31</td>\n",
       "      <td>54.37</td>\n",
       "      <td>41.71</td>\n",
       "      <td>2251.938</td>\n",
       "      <td>2729.715</td>\n",
       "      <td>5.24</td>\n",
       "      <td>WINTER</td>\n",
       "      <td>1</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>304</td>\n",
       "      <td>2009-02-28</td>\n",
       "      <td>52.30</td>\n",
       "      <td>39.09</td>\n",
       "      <td>2074.167</td>\n",
       "      <td>2332.539</td>\n",
       "      <td>4.52</td>\n",
       "      <td>WINTER</td>\n",
       "      <td>1</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>2009-03-31</td>\n",
       "      <td>44.34</td>\n",
       "      <td>47.94</td>\n",
       "      <td>2262.488</td>\n",
       "      <td>2170.709</td>\n",
       "      <td>3.96</td>\n",
       "      <td>WINTER</td>\n",
       "      <td>1</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>365</td>\n",
       "      <td>2009-04-30</td>\n",
       "      <td>41.92</td>\n",
       "      <td>49.65</td>\n",
       "      <td>2147.856</td>\n",
       "      <td>1741.293</td>\n",
       "      <td>3.50</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>0</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Days       Date  AveCoalPrice  OilPrice  GrossGasProd  TotGasCons  \\\n",
       "0   245 2008-12-31         57.22     41.12      2227.028    2399.702   \n",
       "1   276 2009-01-31         54.37     41.71      2251.938    2729.715   \n",
       "2   304 2009-02-28         52.30     39.09      2074.167    2332.539   \n",
       "3   335 2009-03-31         44.34     47.94      2262.488    2170.709   \n",
       "4   365 2009-04-30         41.92     49.65      2147.856    1741.293   \n",
       "\n",
       "   GasPrice Weather  WSTAT GasPriceStatus  GPSAT  color  \n",
       "0      5.82  WINTER      1           HIGH      1      1  \n",
       "1      5.24  WINTER      1           HIGH      1      1  \n",
       "2      4.52  WINTER      1           HIGH      1      1  \n",
       "3      3.96  WINTER      1           HIGH      1      1  \n",
       "4      3.50  SPRING      0           HIGH      1      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#load the data for analysis\n",
    "dflog=pd.read_excel(\"data/DataSet_GasPrice_ Outlier_Removed.xlsx\")\n",
    "dflog.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYS</th>\n",
       "      <th>DATE</th>\n",
       "      <th>COALP</th>\n",
       "      <th>OILP</th>\n",
       "      <th>GPROD</th>\n",
       "      <th>GCONS</th>\n",
       "      <th>GASP</th>\n",
       "      <th>WEATH</th>\n",
       "      <th>WSTAT</th>\n",
       "      <th>GPSTAT</th>\n",
       "      <th>GPSAT</th>\n",
       "      <th>COL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>245</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>57.22</td>\n",
       "      <td>41.12</td>\n",
       "      <td>2227.028</td>\n",
       "      <td>2399.702</td>\n",
       "      <td>5.82</td>\n",
       "      <td>WINTER</td>\n",
       "      <td>1</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>276</td>\n",
       "      <td>2009-01-31</td>\n",
       "      <td>54.37</td>\n",
       "      <td>41.71</td>\n",
       "      <td>2251.938</td>\n",
       "      <td>2729.715</td>\n",
       "      <td>5.24</td>\n",
       "      <td>WINTER</td>\n",
       "      <td>1</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>304</td>\n",
       "      <td>2009-02-28</td>\n",
       "      <td>52.30</td>\n",
       "      <td>39.09</td>\n",
       "      <td>2074.167</td>\n",
       "      <td>2332.539</td>\n",
       "      <td>4.52</td>\n",
       "      <td>WINTER</td>\n",
       "      <td>1</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>335</td>\n",
       "      <td>2009-03-31</td>\n",
       "      <td>44.34</td>\n",
       "      <td>47.94</td>\n",
       "      <td>2262.488</td>\n",
       "      <td>2170.709</td>\n",
       "      <td>3.96</td>\n",
       "      <td>WINTER</td>\n",
       "      <td>1</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>365</td>\n",
       "      <td>2009-04-30</td>\n",
       "      <td>41.92</td>\n",
       "      <td>49.65</td>\n",
       "      <td>2147.856</td>\n",
       "      <td>1741.293</td>\n",
       "      <td>3.50</td>\n",
       "      <td>SPRING</td>\n",
       "      <td>0</td>\n",
       "      <td>HIGH</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DAYS       DATE  COALP   OILP     GPROD     GCONS  GASP   WEATH  WSTAT  \\\n",
       "0   245 2008-12-31  57.22  41.12  2227.028  2399.702  5.82  WINTER      1   \n",
       "1   276 2009-01-31  54.37  41.71  2251.938  2729.715  5.24  WINTER      1   \n",
       "2   304 2009-02-28  52.30  39.09  2074.167  2332.539  4.52  WINTER      1   \n",
       "3   335 2009-03-31  44.34  47.94  2262.488  2170.709  3.96  WINTER      1   \n",
       "4   365 2009-04-30  41.92  49.65  2147.856  1741.293  3.50  SPRING      0   \n",
       "\n",
       "  GPSTAT  GPSAT  COL  \n",
       "0   HIGH      1    1  \n",
       "1   HIGH      1    1  \n",
       "2   HIGH      1    1  \n",
       "3   HIGH      1    1  \n",
       "4   HIGH      1    1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change the columns name to create attributes and features\n",
    "dflog.columns = ['DAYS', 'DATE', 'COALP', 'OILP', 'GPROD', 'GCONS', 'GASP', 'WEATH', 'WSTAT', 'GPSTAT', 'GPSAT', 'COL']\n",
    "dflog.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us start with the relationship that has the most explicit classification: That is classification of Gas price and Gas Consumption with respect to weather Status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into training set and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Split the data into a training and test set.\n",
    "Xlr, Xtestlr, ylr, ytestlr = train_test_split(dflog[['GASP','GCONS']].values, \n",
    "                                              (dflog.WSTAT == 0).values,random_state=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now generate a Random Forest Classifier and fit the training sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = clf.fit(Xlr, ylr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extremely Randomnized Tree\n",
    "In this section we will take the randomness a little  bit further by using extremely randomized trees. In this case, a random subset of candidate features is used, but instead of looking for the most discriminative thresholds, thresholds are drawn at random for each candidate feature and the best of these randomly-generated thresholds is picked as the splitting rule. This usually allows to reduce the variance of the model a bit more, at the expense of a slightly greater increase in bias. \n",
    "\n",
    "Let us import the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a DecisionTree Classifier and estimate the score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65384615384615385"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score (clf, Xlr, ylr)\n",
    "scores.mean() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now generate a Random Forest Classifier and estimate the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62820512820512819"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score (clf, Xlr, ylr)\n",
    "scores.mean() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate an ExtraTrees Classifier and estimate the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.64102564102564108"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score (clf, Xlr, ylr)\n",
    "scores.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean() > 0.65"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "One other popular boosting algorithm is AdaBoost. The core principle of AdaBoost is to fit a sequence of weak learners (i.e., models that are only slightly better than random guessing, such as small decision trees) on repeatedly modified versions of the data. The predictions from all of them are then combined through a weighted majority vote (or sum) to produce the final prediction.\n",
    "\n",
    "Using the natural gas dataset let us fit an AdaBoost classifier with 100 weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the required library\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate AdaBoost Classifier and estimate the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = AdaBoostClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.62820512820512819"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score (clf, Xlr, ylr)\n",
    "scores.mean() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Tree Boosting\n",
    "Gradient Boosted Regression Trees (GBRT) is a generalization of boosting to arbitrary differentiable loss functions. GBRT is an accurate and effective off-the-shelf procedure that can be used for both regression and classification problems. \n",
    "The advantages of GBRT are:\n",
    "1. Natural handling of data of mixed type (= heterogeneous features)\n",
    "2. Predictive power\n",
    "3. Robustness to outliers in output space (via robust loss functions)\n",
    "\n",
    "The primary disadvantage of GBRT is Scalability as due to the sequential nature of boosting, it can hardly be parallelized. Parallelization is the parallel construction of the trees and the parallel computation of the predictions. \n",
    "\n",
    "We will use the sklearn.ensemble module to provide for both classification and regression via gradient boosted regression trees.\n",
    "\n",
    "#### Classification\n",
    "GradientBoostingClassifier supports both binary and multi-class classification. Here we will fit a gradient boosting classifier with 100 decision stumps as weak learners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69230769230769229"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(Xlr, ylr)\n",
    "clf.score(Xtestlr,ytestlr)                 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression\n",
    "\n",
    "GradientBoostingRegressor supports a number of different loss functions for regression which can be specified via the argument loss; the default loss function for regression is least squares ('ls')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the libraries\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now carry out regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20174199668572462"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=1, random_state=0, loss='ls').fit(Xlr, ylr)\n",
    "mean_squared_error(ytestlr, est.predict(Xtestlr))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
